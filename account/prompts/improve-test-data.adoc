= Developer Prompt: Improving Test Cases for AMS

Your task is to enhance an existing test configuration that utilizes YAML files for initializing system state (`_setup.yaml`), specifying input commands (`in.yaml`), and verifying expected outcomes (`out.yaml`).
The goal is to produce maintainable, clear, and requirements-aligned test cases.

== Overview

The testing approach involves the following files:

* `_setup.yaml` - Initialises the system state before the test scenario begins (e.g., creating initial accounts).
* `in.yaml` - Defines the input commands (events) that the system under test will process.
* `out.yaml` - Specifies the expected events produced by the system in response to the inputs, along with helpful comments that link the outputs back to the corresponding input events.

Below is an illustrative structure:

.Setup (`_setup.yaml`)
----
---
# Create account for Alice (account #101013) starting with 15 EUR.
# Rationale: This sets up a baseline account state for subsequent operations.
createAccount: {
  sender: gw1,
  target: vault,
  sendingTime: 2023-01-20T10:00:00,
  name: alice,
  account: 101013,
  currency: EUR,
  balance: 15
}
...
----

.Input (`in.yaml`)
----
---
# Transfer 10 EUR from Alice (101013) to Bob (101025).
# Scenario: This should succeed if Bob’s account is in EUR and both accounts are valid.
transfer: {
  sender: gw2,
  target: vault,
  sendingTime: 2023-01-20T10:03:00,
  from: 101013,
  to: 101025,
  currency: EUR,
  amount: 10,
  reference: Dog food
}
...
---
# This operation requests a checkpoint.
# Checkpoints are typically used to dump or save the state of the system at a certain point in time.
# In this case, it will dump all the accounts.
checkPoint: {
  sender: gw2,
  target: vault,
  sendingTime: 2023-01-20T11:00:00,
}
...
----

[source,mermaid]
----
sequenceDiagram
    participant SetupFile as _setup.yaml
    participant InputFile as in.yaml
    participant TestRunner as YamlTester
    participant System as System Under Test<br> (AMS)
    participant ExpectedOutput as out.yaml

    SetupFile->>TestRunner: Load initial state instructions
    Note over TestRunner: The Test Runner reads _setup.yaml<br>and applies initial configurations<br> (e.g., create accounts)

    TestRunner->>System: Initialise system state<br> from _setup.yaml
    Note over System: System now has initial state<br> (e.g., Alice’s account with 15 EUR)

    InputFile->>TestRunner: Provide input<br> commands/events
    Note over TestRunner: The Test Runner reads in.yaml<br> which includes operations<br> like transfers, checkpoints

    TestRunner->>System: Replay input events<br> from in.yaml
    Note over System: System processes each command<br>and produces corresponding output events<br> (e.g., onTransfer, createAccountFailed)

    System->>TestRunner: Return<br> produced events
    Note over TestRunner: The Test Runner captures all events<br>generated by the system<br> in response to input

    ExpectedOutput->>TestRunner: Provide expected events (out.yaml)
    Note over TestRunner: The Test Runner checks the produced events<br>against the expected output<br> defined in out.yaml

    TestRunner->>TestRunner: Compare actual vs expected events
    alt All Events Match
        TestRunner->>System: Test PASSED
    else Some Events Differ
        TestRunner->>System: Test FAILED
    end
----

== Guidelines

1. **Clarity and Context**:
Add descriptive comments to `_setup.yaml` and `in.yaml` to explain each operation’s intent.
In `out.yaml`, reference the input event that caused the output.
This makes it easier for other developers to understand the test scenarios at a glance.

2. **Time Management**:
Document that real-time tests should use `SystemTimeProvider.CLOCK.currentTimeNanos()` for `sendingTime`.
Though test files may use fixed timestamps, emphasize in comments that production environments rely on `SystemTimeProvider` for consistent, nanosecond-precision timestamps.

3. **Validation Checks**:
Introduce failure scenarios:
* A `createAccount` command with an invalid balance (e.g., negative balance) to produce `createAccountFailed`.
* A `transfer` from a non-existent account or with insufficient funds to produce `transferFailed`.

   In `in.yaml`, comment these scenarios and in `out.yaml`, show the expected failure outputs, including a `reason` field that aligns with the system’s requirements.

4. **Reusability and Maintenance**:
If your setup becomes complex, consider YAML anchors, aliases, or splitting large scenarios into multiple files.
Add comments linking tricky scenarios to relevant sections of the requirements document, ensuring future maintainers understand the rationale behind each test.

5. **Coverage**:
Include scenarios that cover:
* Multiple successful account creations and transfers.
* At least one invalid `createAccount` scenario.
* At least one invalid `transfer` scenario.
* A `checkPoint` command to verify the sequence of `startCheckpoint`, `onCreateAccount` for each known account, and `endCheckpoint` events.

6. **Naming and Organization**:
Use meaningful and specific operation descriptions.
Instead of generic comments, specify the exact accounts, currencies, and reasons.
Label scenarios (e.g., "Scenario: Insufficient Funds Transfer") to quickly identify their purpose.

== Sections for Setup and Input Data

- **Setup Section (`_setup.yaml`)**:
Place all initial state operations here.
Add comments that justify these initial states and their relevance to the upcoming tests.

----
# Example (in `_setup.yaml`):
# Creating initial accounts to ensure subsequent transfers have valid source and destination accounts.
createAccount: { ... }
...

----

- **Input Section (`in.yaml`)**:
Define the sequence of commands tested.
Include both normal and edge cases, clearly tagging scenarios for quick reference.

----
# Example (in `in.yaml`):
# Scenario: Attempt to transfer from a non-existent account to test transferFailed event.
transfer: { ... }
...

----

== Deliverables

Enhance the existing `_setup.yaml` and `in.yaml` files according to the above guidelines.
Once updated, provide a brief summary of the changes made and the reasons behind them, focusing on improved clarity, test coverage, and alignment with requirements.
